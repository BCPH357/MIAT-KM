from neo4j import GraphDatabase
import re
from typing import List, Dict, Tuple
from langchain_neo4j import GraphCypherQAChain
from langchain_neo4j import Neo4jGraph
from langchain_ollama import OllamaLLM
from langchain.prompts.prompt import PromptTemplate
from config import (
    OLLAMA_MODEL,
    OLLAMA_BASE_URL,
    MODEL_TEMPERATURE,
    MODEL_NUM_PREDICT,
    MODEL_TOP_P,
    MODEL_TOP_K,
    CYPHER_GENERATION_PROMPT,
    KNOWLEDGE_GRAPH_QA_PROMPT
)

class Neo4jKnowledgeRetriever:
    def __init__(self, uri: str, user: str, password: str):
        self.driver = GraphDatabase.driver(uri, auth=(user, password))
        
        # åˆå§‹åŒ– LangChain çµ„ä»¶
        self.graph = Neo4jGraph(
            url=uri,
            username=user,
            password=password
        )
        
        # åˆå§‹åŒ– Ollama LLM
        self.llm = OllamaLLM(
            model=OLLAMA_MODEL,
            base_url=OLLAMA_BASE_URL,
            temperature=MODEL_TEMPERATURE,
            num_predict=MODEL_NUM_PREDICT,
            top_p=MODEL_TOP_P,
            top_k=MODEL_TOP_K
        )
        
        # å‰µå»ºè‡ªå®šç¾©çš„ Cypher prompt
        custom_cypher_prompt = PromptTemplate.from_template(CYPHER_GENERATION_PROMPT)

        # å‰µå»ºè‡ªå®šç¾©çš„ QA prompt
        custom_qa_prompt = PromptTemplate.from_template(KNOWLEDGE_GRAPH_QA_PROMPT)

        # å‰µå»º GraphCypherQAChain
        self.cypher_chain = GraphCypherQAChain.from_llm(
            llm=self.llm,
            graph=self.graph,
            cypher_prompt=custom_cypher_prompt,
            qa_prompt=custom_qa_prompt,
            verbose=True,
            return_intermediate_steps=True,
            return_direct=False,
            allow_dangerous_requests=True
        )
    
    def close(self):
        self.driver.close()

    def hybrid_search_context_only(self, query: str) -> Dict:
        """
        åƒ…é€²è¡ŒçŸ¥è­˜åœ–è­œæª¢ç´¢,ä¸ç”ŸæˆLLMå›ç­”
        ç”¨æ–¼hybrid-allæ¨¡å¼,é¿å…é‡è¤‡ç”Ÿæˆå›ç­”
        """
        try:
            # ä½¿ç”¨LangChainåƒ…é€²è¡ŒCypheræŸ¥è©¢å’Œæ•¸æ“šæª¢ç´¢
            result = self.cypher_chain.invoke({"query": query})

            # è§£ææª¢ç´¢çµæœ
            cypher_query = ''
            context_data = []

            if 'intermediate_steps' in result and result['intermediate_steps']:
                if len(result['intermediate_steps']) > 0:
                    cypher_query = result['intermediate_steps'][0].get('query', '')
                if len(result['intermediate_steps']) > 1:
                    context_data = result['intermediate_steps'][1].get('context', [])

            return {
                'cypher_query': cypher_query,
                'context': context_data,
                'mode': 'context_only'
            }
        except Exception as e:
            print(f"âŒ çŸ¥è­˜åœ–è­œæª¢ç´¢éŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            return {
                'cypher_query': '',
                'context': [],
                'mode': 'context_only'
            }

    def hybrid_search(self, query: str, ollama_client) -> Dict:
        """
        æ··åˆRAGæ¨¡å¼ï¼šä½¿ç”¨LangChainæª¢ç´¢ + è‡ªå®šç¾©å›ç­”ç”Ÿæˆ
        """
        try:
            # ä½¿ç”¨LangChainåƒ…é€²è¡ŒCypheræŸ¥è©¢å’Œæ•¸æ“šæª¢ç´¢
            result = self.cypher_chain.invoke({"query": query})
            
            # è§£ææª¢ç´¢çµæœ
            cypher_query = ''
            context_data = []
            
            if 'intermediate_steps' in result and result['intermediate_steps']:
                if len(result['intermediate_steps']) > 0:
                    cypher_query = result['intermediate_steps'][0].get('query', '')
                if len(result['intermediate_steps']) > 1:
                    context_data = result['intermediate_steps'][1].get('context', [])
            
            # æ ¼å¼åŒ–æª¢ç´¢åˆ°çš„çŸ¥è­˜ç”¨æ–¼è‡ªå®šç¾©ç”Ÿæˆ
            if context_data:
                knowledge_items = []
                for i, item in enumerate(context_data, 1):
                    if isinstance(item, dict):
                        subject = item.get('subject', '')
                        predicate = item.get('predicate', '')
                        object_val = item.get('object', '')
                        knowledge_items.append(f"{i}. {subject} â†’ [{predicate}] â†’ {object_val}")
                    else:
                        knowledge_items.append(f"{i}. {item}")
                
                knowledge_context = "å¾çŸ¥è­˜åœ–è­œæª¢ç´¢åˆ°çš„ç›¸é—œä¿¡æ¯ï¼š\n" + "\n".join(knowledge_items)
                
                # ä½¿ç”¨è‡ªå®šç¾©RAGç”Ÿæˆè©³ç´°å›ç­”
                detailed_answer = ollama_client.rag_generate(
                    model=OLLAMA_MODEL,
                    user_query=query,
                    knowledge_context=knowledge_context,
                    temperature=0.7
                )
            else:
                detailed_answer = "å¾ˆæŠ±æ­‰ï¼ŒçŸ¥è­˜åº«ä¸­æ²’æœ‰æ‰¾åˆ°ç›¸é—œä¿¡æ¯ä¾†å›ç­”æ‚¨çš„å•é¡Œã€‚"
            
            print(f"ğŸ” æ··åˆæ¨¡å¼å®Œæ•´çµæœçµæ§‹: {result.keys()}")
            if 'intermediate_steps' in result:
                print(f"ğŸ“Š ä¸­é–“æ­¥é©Ÿæ•¸é‡: {len(result['intermediate_steps'])}")
                for i, step in enumerate(result['intermediate_steps']):
                    print(f"   æ­¥é©Ÿ {i}: {step.keys()}")
            
            return {
                'answer': detailed_answer,
                'cypher_query': cypher_query,
                'context': context_data,
                'full_result': result,
                'raw_intermediate_steps': result.get('intermediate_steps', []),
                'mode': 'hybrid'
            }
        except Exception as e:
            print(f"âŒ æ··åˆæ¨¡å¼æŸ¥è©¢éŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            return {
                'answer': f'æŸ¥è©¢éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}',
                'cypher_query': '',
                'context': [],
                'full_result': {},
                'raw_intermediate_steps': [],
                'mode': 'hybrid'
            }